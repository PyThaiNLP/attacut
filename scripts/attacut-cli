#!/usr/bin/env python

"""AttaCut: Fast and Reasonably Accurate Tokenizer for Thai

Usage:
  attacut-cli <src> [--dest=<dest>] [--model=<model>]
  attacut-cli (-h | --help)

Options:
  -h --help         Show this screen.
  --model=<model>   Model to be used [default: attacut-sc].
  --dest=<dest>     If not specified, it'll be <src>-tokenized-by-<model>.txt

"""

import torch

from docopt import docopt
from tqdm import tqdm

from attacut import utils, artifacts, pipeline, Tokenizer

def get_argument(dict, name, default):
    v = dict.get(name)
    return v if v is not None else default
  
if __name__ == "__main__":
    arguments = docopt(__doc__)

    src = arguments["<src>"]
    model = arguments["--model"]

    # for a custom model, use the last dir's name.
    model_name = model.split("/")[-1]

    dest = get_argument(
        arguments,
        "--dest",
        utils.add_suffix_to_file_path(
            src,
            f"-tokenized-by-{model_name}"
        )
    )

    print(f"Tokenizing {src}")
    print(f"using {model}")
    print(f"Output: {dest}")

    # # todo: fall back to model_dir if None

    tokenizer = Tokenizer(model)
    total_lines = utils.wc_l(src)

    with torch.no_grad(), \
        open(src, "r") as fin, \
        open(dest, "w") as fout:
        # todo: dataloader generator
        # todo: tokenize by batch and write results
        for line in tqdm(fin, total=total_lines):

            line = pipeline.TRAILING_SPACE_RX.sub("", line)

            if not line:
                fout.write("\n")
                continue

            result = tokenizer.tokenize(line)

            fout.write(f"{result}\n")
