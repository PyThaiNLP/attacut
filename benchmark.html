
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Benchmarking &#8212; AttaCut  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Retraining" href="training.html" />
    <link rel="prev" title="Word Tokenization for Thai" href="survey.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="benchmarking">
<span id="sec-benchmark"></span><h1>Benchmarking<a class="headerlink" href="#benchmarking" title="Permalink to this headline">¶</a></h1>
<p>We value reproducibility. Our experiments should be reproducible and expected
to have similar results when one tries. Therefore, we 1) desrcibe our
benchmarking procedure as complete as possible and 2) publish all the code with
fair amount of documentation.</p>
<p>If there is any doubt or unclear part,
please let us know. We are happy to clarify and improve the document.</p>
<p>Please note here that AttaCut models are denoted as <strong>AttaCut-SC</strong> and
<strong>AttaCut-C</strong>. The former is AttaCut with syllable and character features,
while the latter uses only character feature.</p>
<section id="tokenization-quality">
<h2>Tokenization Quality<a class="headerlink" href="#tokenization-quality" title="Permalink to this headline">¶</a></h2>
<p>Tokenization quality is measured in terms of <strong>precision</strong>, <strong>recall</strong>, and
<strong>f1</strong>. We do the measurements in two levels, namely character and word.
Figure below describes how these metrics are computed:</p>
<figure class="align-default" id="id9">
<img alt="_images/evaluation-long.png" src="_images/evaluation-long.png" />
<figcaption>
<p><span class="caption-text">Character- and Word-Level Metrics for Word Tokenization</span><a class="headerlink" href="#id9" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Character</span><span class="o">-</span><span class="n">Level</span><span class="p">:</span>
<span class="p">[</span><span class="n">P</span><span class="p">]</span><span class="n">recision</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span> <span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span> <span class="p">)</span>
<span class="p">[</span><span class="n">R</span><span class="p">]</span><span class="n">ecall</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span> <span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span> <span class="p">)</span>
<span class="n">f1</span> <span class="o">=</span> <span class="mi">2</span><span class="n">PR</span> <span class="o">/</span> <span class="p">(</span><span class="n">P</span><span class="o">+</span><span class="n">R</span><span class="p">)</span>

<span class="n">Word</span><span class="o">-</span><span class="n">Level</span><span class="p">:</span>
<span class="n">P</span> <span class="o">=</span> <span class="c1">#✓ / #◼︎ in prediction</span>
<span class="n">R</span> <span class="o">=</span> <span class="c1">#✓ / #◼︎ in text</span>
</pre></div>
</div>
<p>To increase reproducibility and ease further research, we have developed an
evaluation framework for this process. The framework contains two main
ingredients:</p>
<ol class="arabic">
<li><div class="line-block">
<div class="line"><strong>Bechmark CLI</strong></div>
<div class="line">At the moment, this CLI can be found at <a class="reference external" href="https://github.com/PyThaiNLP/tokenization-benchmark">&#64;pythainlp’s tokenization-benchmark</a>, but it will be soon released in the main PyThaiNLP package (version 2.1). Please see it this milestone <a class="footnote-reference brackets" href="#milestone" id="id1">1</a> for recent updates.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Result Visualization and Comparison Website</strong></div>
<div class="line">This website serves as a tool for error analysis on tokenization results as well as a benchmark collection of other publicly available tokenizers.</div>
</div>
<blockquote>
<div><figure class="align-default" id="id10">
<img alt="https://camo.githubusercontent.com/85984f46bb0db3e2bb86b16969b570b7faf4535a/68747470733a2f2f692e696d6775722e636f6d2f56564159485a4d2e706e67" src="https://camo.githubusercontent.com/85984f46bb0db3e2bb86b16969b570b7faf4535a/68747470733a2f2f692e696d6775722e636f6d2f56564159485a4d2e706e67" />
<figcaption>
<p><span class="caption-text">Tokenization Benchmark Visualization <a class="footnote-reference brackets" href="#viz" id="id2">2</a></span><a class="headerlink" href="#id10" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</div></blockquote>
</li>
</ol>
<section id="results">
<h3>Results <a class="footnote-reference brackets" href="#benchsheet" id="id3">3</a><a class="headerlink" href="#results" title="Permalink to this headline">¶</a></h3>
<p>We evaluate tokenization quality on four datasets, namely BEST <a class="footnote-reference brackets" href="#best" id="id4">4</a>, Orchid <a class="footnote-reference brackets" href="#orchid" id="id5">6</a>,
1000 samples from Wisesight Sentiment Corpus <a class="footnote-reference brackets" href="#wisesight-tok" id="id6">8</a>, and Thai National Historical Corpus (TNHC) <a class="footnote-reference brackets" href="#tnhc" id="id7">5</a>.</p>
<p>Because we train on BEST, Orchid, Wisesight, and TNHC are
out-domain evaluations, testing whether tokenizers are robust.</p>
<figure class="align-default" id="id11">
<img alt="_images/quality-benchmark-in-of-domain.png" src="_images/quality-benchmark-in-of-domain.png" />
<figcaption>
<p><span class="caption-text">Tokenization Quality on BEST (in-domain)</span><a class="headerlink" href="#id11" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id12">
<img alt="_images/quality-benchmark-out-of-domain.png" src="_images/quality-benchmark-out-of-domain.png" />
<figcaption>
<p><span class="caption-text">Tokenization Quality on Wisesight, Orchid, and TNHC (out-domain)</span><a class="headerlink" href="#id12" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>For in-domain evaluations, AttaCut-SC’s quality is quite similar
to DeepCut only two percentage different on BEST’s test set. On the other hand,
ML-based tokenizers are on par on Wisesight 1000-sample set. Interestingly,
on Orchid and TNHC, PyThaiNLP’s newmm is the best. The reason might be that
these two datasets use a different tokenization standard than BEST.</p>
</section>
</section>
<section id="speed">
<h2>Speed<a class="headerlink" href="#speed" title="Permalink to this headline">¶</a></h2>
<p>Our speed benchmarking is done on standardized environments, namely Google Colab
and AWS’s EC2 instances (t2.small &amp; t2.medium).</p>
<section id="benchmarking-on-google-colab">
<h3>Benchmarking on Google Colab<a class="headerlink" href="#benchmarking-on-google-colab" title="Permalink to this headline">¶</a></h3>
<p>Due to Google Colab’s accessibilty and convenience, we use Google Colab for our
early speed benchmarking. In this experiment, we vary the length of input text
and measure the speed of tokenizers. From the figure below, we can see that
our AttaCut models are significantly faster than DeepCut.</p>
<figure class="align-default" id="id13">
<img alt="_images/colab-speed-benchmark.png" src="_images/colab-speed-benchmark.png" />
<figcaption>
<p><span class="caption-text">Tokenization Time of PyThaiNLP’s newmm, DeepCut, and AttaCut on Google Colab</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
</section>
<section id="benchmarking-on-ec2-instances">
<h3>Benchmarking on EC2 Instances<a class="headerlink" href="#benchmarking-on-ec2-instances" title="Permalink to this headline">¶</a></h3>
<p>Practically, tokenization is part of NLP pipelines that is usually done on
cloud instances, such as AWS EC2, due to scalibility and cost efficiency.
Typically, these instances contain a couple of CPU cores and memory,
posing another challenge to services, i.e. tokenization, executued there.</p>
<p>Evaluating tokenizer’s speed on such an instance allows us to get realistic
results and yet reproducible. We use the training set of Wisesight Sentiment
Corpus <a class="footnote-reference brackets" href="#wisesight" id="id8">7</a> as a input dataset. The corpus contains texts from social
media and online forum platforms. The training set has around 24,000 lines and
about 1.5M characters.</p>
<figure class="align-default" id="id14">
<img alt="_images/speed-benchmark-ec2.png" src="_images/speed-benchmark-ec2.png" />
<figcaption>
<p><span class="caption-text">Wisesight’s Training Set Tokenization Time of PyThaiNLP’s newmm, DeepCut, and AttaCut on AWS Instances.</span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>From the figure above, AttaCut models are fasters than other existing ML-based
tokenizers. More precisely, <strong>AttaCut-SC</strong> (our best model) is aroud <strong>6x</strong>
faster than <strong>DeepCut</strong>, the current state of the art word tokenizer for Thai,
while having a similar level of tokenization quality.</p>
<dl class="footnote brackets">
<dt class="label" id="milestone"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/PyThaiNLP/pythainlp/milestone/11">PyThaiNLP 2.1 Milestone</a></p>
</dd>
<dt class="label" id="viz"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://pythainlp.github.io/tokenization-benchmark-visualization/">Tokenization Benchmark Visualization</a></p>
</dd>
<dt class="label" id="benchsheet"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://docs.google.com/spreadsheets/d/1hata1Y1C-j8p_d3-kJzqy6ENfNNWP195qEz08u0uFhQ/edit?usp=sharing">P. Chormai. Tokenization Quality Benchmark SpreadSheet, 2019</a></p>
</dd>
<dt class="label" id="best"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p>NECTEC. BEST: Benchmark for Enhancing the Standard of Thai language processing, 2010.</p>
</dd>
<dt class="label" id="tnhc"><span class="brackets"><a class="fn-backref" href="#id7">5</a></span></dt>
<dd><p><a class="reference external" href="https://attapol.github.io/tlc.html">J. Sawatphol and A. Rutherford. TNHC: Thai National Historical Corpus, 2019.</a></p>
</dd>
<dt class="label" id="orchid"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p><a class="reference external" href="https://www.semanticscholar.org/paper/ORCHID-%3A-Thai-Part-Of-Speech-Tagged-Corpus-Sornlertlamvanich-Charoenporn/f9f8dc979727e3a31c4cedcbdfad9523c28c009f">V. Sornlertlamvanich et al. ORCHID: Thai Part-Of-Speech Tagged Corpus, 2009</a></p>
</dd>
<dt class="label" id="wisesight"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/PyThaiNLP/wisesight-sentiment">PyThaiNLP. Wisesight-Sentiment Corpus, 2019</a></p>
</dd>
<dt class="label" id="wisesight-tok"><span class="brackets"><a class="fn-backref" href="#id6">8</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/PyThaiNLP/wisesight-sentiment/tree/master/word-tokenization">PyThaiNLP. 1000 Samples from Wisesight-Sentiment Corpus, 2019</a></p>
</dd>
</dl>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/logo.png" alt="Logo"/>
    
  </a>
</p>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">NLP 101</a></li>
<li class="toctree-l1"><a class="reference internal" href="survey.html">Word Tokenization for Thai</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Benchmarking</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#tokenization-quality">Tokenization Quality</a></li>
<li class="toctree-l2"><a class="reference internal" href="#speed">Speed</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Retraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="misc.html">Other Links</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="survey.html" title="previous chapter">Word Tokenization for Thai</a></li>
      <li>Next: <a href="training.html" title="next chapter">Retraining</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Pattarawat Chormai et al..
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.1.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/benchmark.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-48736618-9']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>