
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Word Tokenization for Thai &#8212; AttaCut  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Benchmarking" href="benchmark.html" />
    <link rel="prev" title="Natural Language Processing 101" href="overview.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="word-tokenization-for-thai">
<h1>Word Tokenization for Thai<a class="headerlink" href="#word-tokenization-for-thai" title="Permalink to this headline">¶</a></h1>
<p>Research in word tokenization for Thai started around 1990. Over these 20 years,
there have been sevaral algorithms being prosed to address the problem. These algorithms
can be clustered into two categories, namely</p>
<ol class="arabic">
<li><div class="line-block">
<div class="line"><strong>Dictionary-based:</strong></div>
<div class="line">Algorithms in this category rely on the use of dictionaries with a mechanism to decide whether to tokenize a particular sequence of characters. Some of algorithms are Chrome’s v8BreakIterator <a class="footnote-reference brackets" href="#icu" id="id1">1</a>  and PyThaiNLP’s newmm <a class="footnote-reference brackets" href="#newmm" id="id2">2</a>.</div>
</div>
</li>
<li><div class="line-block">
<div class="line"><strong>Learning-based:</strong></div>
<div class="line">Unlike dictionary-based, algorithms in this group learn to split words based on labelled data. The learning problem is typically formulated as <strong>binary classification</strong> on sequence of characters.</div>
</div>
<div class="figure align-center" id="id13">
<a class="reference internal image-reference" href="_images/binary-classification.png"><img alt="_images/binary-classification.png" src="_images/binary-classification.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-text">Binary Classification for Word Tokenization. <strong>B</strong> denotes a starting-word character, while <strong>I</strong> represents the opposite.</span><a class="headerlink" href="#id13" title="Permalink to this image">¶</a></p>
</div>
<div class="line-block">
<div class="line">With the rise of neural networks, recent developments of Thai tokenizers are either Convolutional Neural Networks (CNNs) (i.e. DeepCut <a class="footnote-reference brackets" href="#deepcut" id="id3">3</a>) or Recurrent Neural Networks (RNNs) (i.e. <a class="footnote-reference brackets" href="#multicut" id="id4">4</a>, <a class="footnote-reference brackets" href="#cantok" id="id5">5</a>, Sertis’ Bi-GRUs <a class="footnote-reference brackets" href="#sertis" id="id6">6</a>).</div>
</div>
</li>
</ol>
<p>Generally, these categories have different advantages and disadvantages.
Dictionary-based algorithms are typically fast but with less capable when encountering unknown words.
On the other hand, learning-based approaches are usually qualitatively better and more adaptable to data from different domains; however, their computation is relatively slower.
Figure below summarizes current solutions into two axes: <strong>Quality (Word-Level f1)</strong> and <strong>Inference time</strong>.</p>
<blockquote>
<div><div class="figure align-center" id="id14">
<img alt="_images/previous-work-spectrum.png" src="_images/previous-work-spectrum.png" />
<p class="caption"><span class="caption-text">Quality and Inference Time of Existing Thai Word Tokenizers. Please see <a class="reference internal" href="benchmark.html#sec-benchmark"><span class="std std-ref">Benchmarking</span></a> for details of evaluation metrics. Device Specification <a class="footnote-reference brackets" href="#id12" id="id7">*</a></span><a class="headerlink" href="#id14" title="Permalink to this image">¶</a></p>
</div>
</div></blockquote>
<p>From the plot, we can see that the DeepCut is state-of-the-art. However,
it is significantly slower than other algorithms.
While PyThaiNLP’s newmm is the the fastest one, lowest inference speed,
it has the lowest tokenization quality.</p>
<p>Because tokenization is usually the first stage in NLP pipelines,
the efficiency and accuracy of the tokenizer are crucial towards building a
successful NLP application. Now, a question arises whether we can build fast
and yet accurate tokenizer for Thai.</p>
<div class="section" id="analysis-of-deepcut">
<h2>Analysis of Deepcut<a class="headerlink" href="#analysis-of-deepcut" title="Permalink to this headline">¶</a></h2>
<p>DeepCut is a CNN with 13 different widths of convolution on character features or embeddings.
Pooling is then used to combined features from these convotional layers, yeiding output to a fully-connected layer for final prediction.</p>
<blockquote>
<div><div class="figure align-center">
<img alt="https://user-images.githubusercontent.com/1214890/58486992-14c1d880-8191-11e9-9122-8385750e06bd.png" src="https://user-images.githubusercontent.com/1214890/58486992-14c1d880-8191-11e9-9122-8385750e06bd.png" />
</div>
</div></blockquote>
<p>In total, DeepCut has around 500,000 trainable variables. Looking at DeepCut’s
architecture, convotion layers are
significatnly overlapped to each other; hence, these layers are redudant.
With this observation, we have a hyphothesis that DeepCut could be smaller
while achieving a similar level of tokenization quality.</p>
<div class="figure align-default" id="id15">
<img alt="_images/deepcut-experiment.png" src="_images/deepcut-experiment.png" />
<p class="caption"><span class="caption-text">Finding which layers in DeepCut could be removed.</span><a class="headerlink" href="#id15" title="Permalink to this image">¶</a></p>
</div>
<p>We design an experiment to verify this proposition. The experiment can be
described as follows:</p>
<ol class="arabic simple">
<li><p>Disable neurons in a layer by zeroing their activitions</p></li>
<li><p>Observe the change of tokenization quality</p></li>
<li><p>Repeat 1-2. to other layers</p></li>
</ol>
<div class="figure align-default" id="id16">
<a class="reference internal image-reference" href="_images/deepcut-experiment-result.png"><img alt="_images/deepcut-experiment-result.png" src="_images/deepcut-experiment-result.png" style="width: 500px;" /></a>
<p class="caption"><span class="caption-text">Speed Comparision between original DeepCut and shrinked models</span><a class="headerlink" href="#id16" title="Permalink to this image">¶</a></p>
</div>
<p>In short, we found that if we remove the convolution layers of kernel width 7,
9, and 10, DeepCut will be 20% faster while its quality drops only 6%. Complete
analysis can be found from our notebook <a class="footnote-reference brackets" href="#deepcutana" id="id8">7</a>. This result confirms
our hyphothesis that we can have a smaller CNN model that is compentent in
speed and quality.</p>
</div>
<div class="section" id="how-does-attacut-look-like">
<h2>How does AttaCut look like?<a class="headerlink" href="#how-does-attacut-look-like" title="Permalink to this headline">¶</a></h2>
<p>Recent trends in NLP have started to shift towards the use of attention and
convolutional models (i.e. Transformer <a class="footnote-reference brackets" href="#attention" id="id9">8</a>). One of the reasons is
due to the fact that the computation of CNNs can be parallelized, while this is
blocked in RNNs because of recurrence dependencies in those models. Hence,
the inference process of CNNs is usually faster than RNNs.</p>
<div class="figure align-default" id="id17">
<img alt="_images/attacut-convolution.png" src="_images/attacut-convolution.png" />
<p class="caption"><span class="caption-text">AttaCut’s three convolutions with different filter widths and dilation rates.</span><a class="headerlink" href="#id17" title="Permalink to this image">¶</a></p>
</div>
<p>Together with our DeepCut analysis, we firstly constuct AttaCut using CNNs.
AttaCut’s convolutional layers are chosen in such a way that overlapping
between filters is minimal while covering a similar context as in DeepCut.
We satisfy this requirement using the dilated convolutions.
Figure below are AttaCut’s convolutions:</p>
<div class="figure align-default" id="id18">
<img alt="_images/syllable-char-embedding.png" src="_images/syllable-char-embedding.png" />
<p class="caption"><span class="caption-text">AttaCut’s Architecture: 3 Convolutions -&gt; Pooling -&gt; Fully-connected Layer</span><a class="headerlink" href="#id18" title="Permalink to this image">¶</a></p>
</div>
<p>Secondly, lingustically, word boundary is always syllable boundary; hence, we
incorporate syllable knowledge into AttaCut. The collaboration is done in a
form of syllable embedding in which characters in the same syllable have
the same syllable embedding. This augmentation contextualizes each character to
have different embeddings depending on its neighbours.</p>
<div class="figure align-default" id="id19">
<a class="reference internal image-reference" href="_images/word-syllable-boundary.png"><img alt="_images/word-syllable-boundary.png" src="_images/word-syllable-boundary.png" style="width: 300px;" /></a>
<p class="caption"><span class="caption-text">Syllable and Word Boundaries</span><a class="headerlink" href="#id19" title="Permalink to this image">¶</a></p>
</div>
<p>For syllable tokenization, we use P. Prasertsom et al’s Syllable SeGmenter
(SSG) <a class="footnote-reference brackets" href="#ssg" id="id10">9</a>. SSG uses Conditional Random Fields (CRFs) on character features.
The released model is trained on Thai National Corpus <a class="footnote-reference brackets" href="#tnc" id="id11">10</a>.</p>
<p class="rubric">References</p>
<dl class="footnote brackets">
<dt class="label" id="icu"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>International Components for Unicode (ICU) BreakIterator</p>
</dd>
<dt class="label" id="newmm"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p><a class="reference external" href="https://www.researchgate.net/publication/243659316_Word_segmentation_for_Thai_in_machine_translation_system">V. Sornlertlamvanich. Word segmentation for Thai in machine translation system. Machine Translation, NECTEC, pages 556–561, 1993.</a></p>
</dd>
<dt class="label" id="deepcut"><span class="brackets"><a class="fn-backref" href="#id3">3</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/rkcosmos/deepcut">R. Kittinaradorn. DeepCut, 2017.</a></p>
</dd>
<dt class="label" id="multicut"><span class="brackets"><a class="fn-backref" href="#id4">4</a></span></dt>
<dd><p><a class="reference external" href="https://www.researchgate.net/publication/327516094_Multi-Candidate_Word_Segmentation_using_Bi-directional_LSTM_Neural_Networks">T. Lapjaturapit, K. Viriyayudhakom, and T. Theeramunkong. Multi-Candidate Word Segmentation using Bi-directional LSTM Neural Networks. pages 1–6, 2018.</a></p>
</dd>
<dt class="label" id="cantok"><span class="brackets"><a class="fn-backref" href="#id5">5</a></span></dt>
<dd><p><a class="reference external" href="https://www.researchgate.net/figure/Variational-LSTM-CRF-model-for-Thai-Named-Entity-Recognition_fig1_329766827">C. Udomcharoenchaikit, P. Vateekul, and P. Boonkwan. Thai Named-Entity Recognition Using Variational Long Short-Term Memory with Conditional Random Field: Selected Revised Papers from the Joint International Symposium on Artificial Intelligence and Natural Language Processing (iSAI-NLP 2017). pages 82–92. 2019.</a></p>
</dd>
<dt class="label" id="sertis"><span class="brackets"><a class="fn-backref" href="#id6">6</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/sertiscorp/thai-word-segmentation">Sertis Corp. Thai word segmentation with bi-directional RNN</a></p>
</dd>
<dt class="label" id="deepcutana"><span class="brackets"><a class="fn-backref" href="#id8">7</a></span></dt>
<dd><p><a class="reference external" href="https://colab.research.google.com/drive/1Kb_Fhh6bS0sC2k3ovi2ce8AaWqFXNgIT">P. Chormai. Analysis of DeepCut</a></p>
</dd>
<dt class="label" id="attention"><span class="brackets"><a class="fn-backref" href="#id9">8</a></span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a></p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id7">*</a></span></dt>
<dd><p>For this experiment, we measured the inference time on MacBook Pro (Retina, 15”, Mid 2015), Intel Core i7 &#64; 2.2 Hz, Memory 16 GB with macOS 10.13.6.</p>
</dd>
<dt class="label" id="ssg"><span class="brackets"><a class="fn-backref" href="#id10">9</a></span></dt>
<dd><p><a class="reference external" href="https://github.com/ponrawee/ssg">P. Prasertsom. Syllable Segmenter for Thai using Conditional Random Fields, 2019.</a></p>
</dd>
<dt class="label" id="tnc"><span class="brackets"><a class="fn-backref" href="#id11">10</a></span></dt>
<dd><ol class="upperalpha simple" start="23">
<li><p>Aroonmanakun, K. Tansiri, and P. Nittayanuparp. Thai National Corpus. pages 153–158, 2009.</p></li>
</ol>
</dd>
</dl>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<p class="logo">
  <a href="index.html">
    <img class="logo" src="_static/logo.png" alt="Logo"/>
    
  </a>
</p>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">NLP 101</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Word Tokenization for Thai</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#analysis-of-deepcut">Analysis of Deepcut</a></li>
<li class="toctree-l2"><a class="reference internal" href="#how-does-attacut-look-like">How does AttaCut look like?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">Benchmarking</a></li>
<li class="toctree-l1"><a class="reference internal" href="training.html">Retraining</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgement.html">Acknowledgement</a></li>
<li class="toctree-l1"><a class="reference internal" href="faqs.html">FAQs</a></li>
<li class="toctree-l1"><a class="reference internal" href="misc.html">Other Links</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="overview.html" title="previous chapter">Natural Language Processing 101</a></li>
      <li>Next: <a href="benchmark.html" title="next chapter">Benchmarking</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2019, Pattarawat Chormai et al..
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.2.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/survey.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
    <script type="text/javascript">

      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-48736618-9']);
      _gaq.push(['_setDomainName', 'none']);
      _gaq.push(['_setAllowLinker', true]);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();

    </script>
    
  </body>
</html>